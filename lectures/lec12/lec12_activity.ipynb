{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load packages \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "## nltk imports\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## uncomment and download if this is your first time running \n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "## sentiment analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "## spacy --- if you get an error at the load step\n",
    "## ! python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "## specify to print all output in a call\n",
    "## and not just first\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_upper</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>CLEAN &amp; QUIET APT HOME BY THE PARK</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>SKYLIT MIDTOWN CASTLE</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3647</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3831</td>\n",
       "      <td>Cozy Entire Floor of Brownstone</td>\n",
       "      <td>COZY ENTIRE FLOOR OF BROWNSTONE</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5022</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              name  \\\n",
       "0  2539                Clean & quiet apt home by the park   \n",
       "1  2595                             Skylit Midtown Castle   \n",
       "2  3647               THE VILLAGE OF HARLEM....NEW YORK !   \n",
       "3  3831                   Cozy Entire Floor of Brownstone   \n",
       "4  5022  Entire Apt: Spacious Studio/Loft by central park   \n",
       "\n",
       "                                         name_upper neighbourhood_group  price  \n",
       "0                CLEAN & QUIET APT HOME BY THE PARK            Brooklyn    149  \n",
       "1                             SKYLIT MIDTOWN CASTLE           Manhattan    225  \n",
       "2               THE VILLAGE OF HARLEM....NEW YORK !           Manhattan    150  \n",
       "3                   COZY ENTIRE FLOOR OF BROWNSTONE            Brooklyn     89  \n",
       "4  ENTIRE APT: SPACIOUS STUDIO/LOFT BY CENTRAL PARK           Manhattan     80  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48895 entries, 0 to 48894\n",
      "Data columns (total 5 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   id                   48895 non-null  int64 \n",
      " 1   name                 48879 non-null  object\n",
      " 2   name_upper           48879 non-null  object\n",
      " 3   neighbourhood_group  48895 non-null  object\n",
      " 4   price                48895 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "## if working from within the repo, can use this relative path\n",
    "data_path = \"../../public_data/airbnb_text.csv\"\n",
    "\n",
    "## load data\n",
    "ab = pd.read_csv(data_path)\n",
    "ab.head()\n",
    "ab.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 1: look for a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_mention_cozy</th>\n",
       "      <th>mention_cozy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bronx</th>\n",
       "      <td>89.231088</td>\n",
       "      <td>73.768000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn</th>\n",
       "      <td>128.175441</td>\n",
       "      <td>91.155458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manhattan</th>\n",
       "      <td>204.109775</td>\n",
       "      <td>129.647171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Queens</th>\n",
       "      <td>102.596682</td>\n",
       "      <td>80.344388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staten Island</th>\n",
       "      <td>120.650307</td>\n",
       "      <td>74.319149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     no_mention_cozy  mention_cozy\n",
       "neighbourhood_group                               \n",
       "Bronx                      89.231088     73.768000\n",
       "Brooklyn                  128.175441     91.155458\n",
       "Manhattan                 204.109775    129.647171\n",
       "Queens                    102.596682     80.344388\n",
       "Staten Island             120.650307     74.319149"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using the `name_upper` var, look at where reviews mention cozy\n",
    "ab['is_cozy'] = ab.name_upper.str.contains(\"COZY\")\n",
    "\n",
    "## find the mean price by neighborhood and whether cozy\n",
    "mp = ab.groupby(['is_cozy', 'neighbourhood_group'])['price'].mean().reset_index()\n",
    "\n",
    "## reshape to wide format so that each borough is row\n",
    "## and one col is the mean price for listings that describe\n",
    "## the place as cozy; other col is mean price for listings\n",
    "## without that word\n",
    "mp_wide = pd.pivot_table(mp, \n",
    "                        index = ['neighbourhood_group'],\n",
    "                        columns = ['is_cozy'])\n",
    "\n",
    "mp_wide.columns = ['no_mention_cozy', 'mention_cozy']\n",
    "\n",
    "mp_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual approach 2: score based on dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## construct dictionary\n",
    "space_indicators = {\n",
    "  'small': ['COZY', 'COMFY', 'LITTLE', 'SMALL'],\n",
    "  'large': ['SPACIOUS', 'LARGE', 'HUGE', 'GIANT']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each listing, find the number of occurrences of words in each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COZY', 'LITTLE']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### first, let's test with one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "\n",
    "### splitting that string at space and looking at overlap with each key\n",
    "### first, look at overlap with the list containing words for small\n",
    "words_overlap_small = [word\n",
    "                       for word in practice_listing.split(\" \") \n",
    "                       if word in space_indicators['small']]\n",
    "words_overlap_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### then, look at overlap with the list containing words for large\n",
    "words_overlap_large = [word \n",
    "                       for word in practice_listing.split(\" \") \n",
    "                       if word in space_indicators['large']]\n",
    "words_overlap_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### could then take length as a fraction of all words\n",
    "len(words_overlap_small)/len(practice_listing.split(\" \"))\n",
    "len(words_overlap_large)/len(practice_listing.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a chill apt next to the subway in LES Chinatown'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## specify example\n",
    "example_for_tag = \"This is a chill apt next to the subway in LES Chinatown\"\n",
    "example_for_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'chill',\n",
       " 'apt',\n",
       " 'next',\n",
       " 'to',\n",
       " 'the',\n",
       " 'subway',\n",
       " 'in',\n",
       " 'LES',\n",
       " 'Chinatown']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try part of speech tagging using nltk\n",
    "tokens = word_tokenize(example_for_tag) # Generate list of tokens\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('chill', 'NN'),\n",
       " ('apt', 'JJ'),\n",
       " ('next', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('subway', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('LES', 'NNP'),\n",
       " ('Chinatown', 'NNP')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_pos = pos_tag(tokens) # generate part of speech tags for those tokens\n",
    " \n",
    "## returns a list of tuples\n",
    "## first element in tuple is a word\n",
    "## second element in tuple is the part of speech\n",
    "# for one_tok in tokens_pos:\n",
    " #   print(one_tok)\n",
    "tokens_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tags are from [Penn Treebank Part-of-Speech tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), some examples:\n",
    "\n",
    "| Tag | Part of Speech        | Description                              | Examples                   |\n",
    "|-----|------------------------|------------------------------------------|-----------------------------|\n",
    "| NN  | Noun, singular or mass | Common noun                              | dog, car, freedom           |\n",
    "| NNP | Proper noun, singular  | Specific name                            | John, London, Google        |\n",
    "| DT  | Determiner             | Introduces a noun                        | the, a, an, this            |\n",
    "| VBZ | Verb, 3rd person singular present | Present tense verb (3rd-person singular) | runs, eats, is              |\n",
    "| JJ  | Adjective              | Describes a noun                         | happy, blue, tall           |\n",
    "| TO  | “to”                   | Infinitive marker or preposition         | to (go), to (school)        |\n",
    "| IN  | Preposition or subordinating conjunction | Shows relation or introduces clause | in, on, at, because         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LES', 'Chinatown']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['chill', 'apt', 'next', 'subway']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use list iteration to extract proper nouns (NNP)\n",
    "## i'm first checking if the second element in the tuple\n",
    "## is equal to NNP\n",
    "## if so, i'm returning the first element in the tuple (the \n",
    "## actual word)\n",
    "all_prop_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"NNP\"]\n",
    "all_prop_noun\n",
    "\n",
    "all_adj_noun = [one_tok[0] for one_tok in tokens_pos \n",
    "                if one_tok[1] == \"JJ\" or \n",
    "               one_tok[1] == \"NN\"]\n",
    "all_adj_noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a recent article\n",
    "article = \"\"\"\n",
    "Faculty Draw the Line: Dartmouth Pushes Back on Federal Compact\n",
    "by Noah Sullivan, October 13, 2025, appeared on The Dartmouth Independent\n",
    "It’s not every day that over five hundred professors at Dartmouth speak with one voice. But that’s what happened this October, when more than half the faculty signed a petition asking President Sian Leah Beilock to reject a federal compact tied to the Trump administration. The deal, pitched as a way to boost funding for research and infrastructure, comes with strings attached. Those strings, according to the petition, would cut into Dartmouth’s ability to govern itself and protect academic freedom. That’s a line many here aren’t willing to cross.\n",
    "The petition didn’t mince words. It said the compact would “undermine” the College’s independence and open the door to political interference. Faculty from across departments signed on, from physics to philosophy. Some said they’d never seen this kind of unity before. Others said it was long overdue. Either way, the message was clear: don’t trade values for money.\n",
    "President Beilock responded with a campus-wide email. She said Dartmouth would never compromise its academic freedom or self-governance. She also said the College was still reviewing the compact and hadn’t made a final decision. That didn’t quiet the conversation. If anything, it made folks more curious. What’s in this compact? Why was Dartmouth one of the nine schools asked to sign? And what happens if we say no?\n",
    "The compact, officially called the “Compact for Academic Excellence in Higher Education,” lays out a set of federal expectations. Schools that sign agree to certain admissions policies, student life rules, and curriculum standards. In return, they get priority access to federal grants and partnerships. Critics say it’s a backdoor way to control what colleges teach and who they admit. Supporters argue it’s a way to make sure taxpayer money goes to schools that meet national goals.\n",
    "At Dartmouth, the debate isn’t just about policy. It’s about identity. This is a place that prides itself on independence. Faculty govern their own departments. Students shape campus life. Decisions are made locally, not handed down from Washington. That’s part of what makes Dartmouth, well, Dartmouth. So when a deal comes along that could change that, people pay attention.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "spacy_article = nlp(article)\n",
    "print(type(spacy_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Faculty Draw the Line: Dartmouth Pushes Back; NER tag: WORK_OF_ART\n",
      "Entity: Federal Compact; NER tag: ORG\n",
      "Entity: Noah Sullivan; NER tag: PERSON\n",
      "Entity: October 13, 2025; NER tag: DATE\n",
      "Entity: The Dartmouth Independent; NER tag: ORG\n",
      "Entity: over five hundred; NER tag: CARDINAL\n",
      "Entity: Dartmouth; NER tag: ORG\n",
      "Entity: one; NER tag: CARDINAL\n",
      "Entity: this October; NER tag: DATE\n",
      "Entity: more than half; NER tag: CARDINAL\n",
      "Entity: Sian Leah Beilock; NER tag: PERSON\n",
      "Entity: Trump; NER tag: PERSON\n",
      "Entity: Dartmouth; NER tag: ORG\n",
      "Entity: College; NER tag: ORG\n",
      "Entity: Beilock; NER tag: PERSON\n",
      "Entity: Dartmouth; NER tag: ORG\n",
      "Entity: College; NER tag: ORG\n",
      "Entity: Dartmouth; NER tag: ORG\n",
      "Entity: nine; NER tag: CARDINAL\n",
      "Entity: Compact for Academic Excellence in Higher Education; NER tag: WORK_OF_ART\n",
      "Entity: Dartmouth; NER tag: ORG\n",
      "Entity: Washington; NER tag: GPE\n",
      "Entity: Dartmouth; NER tag: ORG\n",
      "Entity: Dartmouth; NER tag: ORG\n"
     ]
    }
   ],
   "source": [
    "## try a couple variations\n",
    "for one_tok in spacy_article.ents:\n",
    "    print(\"Entity: \" + one_tok.text + \"; NER tag: \" + one_tok.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1\n",
    "\n",
    "Play around with other texts, and look at the results. For instance, try the following:\n",
    "\n",
    "- What happens if you use Tweets with hashtags and emojis?\n",
    "- What happens if you have poorly formatted text, or words that can mean different things?\n",
    "- What entities were misclassified in your experiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2\n",
    "\n",
    "How do we generalize from doing Named Entity Recognition one just one string to doing NER on a whole column that contains strings? By defining and executing a **function**, of course!\n",
    "\n",
    "Using the following sample of strings from airbnb listing names, define a function that takes in one airbnb string and does the following:\n",
    "\n",
    "- iterates over entities in that string (list comprehension would work well for this)\n",
    "- checks if each label indicates a place\n",
    "- returns the text of each original place/entity\n",
    "\n",
    "Then execute the function on the example strings through iteration (again, list comprehension works well here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10                    Beautiful 1br on Upper West Side\n",
       "11                     Central Manhattan/near Broadway\n",
       "12      Lovely Room 1, Garden, Best Area, Legal rental\n",
       "13    Wonderful Guest Bedroom in Manhattan for SINGLES\n",
       "14                       West Village Nest - Superhost\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for runtime purposes, take a sample of the airbnb listing names\n",
    "ab_name_examples = ab.name[10:15]\n",
    "ab_name_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your function code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the default scorer on a few example phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'vaderSentiment.vaderSentiment.SentimentIntensityAnalyzer'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## initialize a scorer\n",
    "sent_obj = SentimentIntensityAnalyzer()\n",
    "print(type(sent_obj))\n",
    "## score one listing\n",
    "practice_listing = \"NICE AND COZY LITTLE APT AVAILABLE\"\n",
    "sentiment_example = sent_obj.polarity_scores(practice_listing)\n",
    "sentiment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase with word terrible and score\n",
    "practice_listing_2 = \"NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW.\"\n",
    "sentiment_example_2 = sent_obj.polarity_scores(practice_listing_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding phrase about rats; bad but might not be in scoring dictionary\n",
    "practice_listing_3 = \"NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH.\"\n",
    "sentiment_example_3 = sent_obj.polarity_scores(practice_listing_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: NICE AND COZY LITTLE APT AVAILABLE scored as:\n",
      "{'neg': 0.0, 'neu': 0.641, 'pos': 0.359, 'compound': 0.4215}\n",
      "String: NICE AND COZY LITTLE APT AVAILABLE. REALLY TERRIBLE VIEW. scored as:\n",
      "{'neg': 0.257, 'neu': 0.531, 'pos': 0.212, 'compound': -0.1513}\n",
      "String: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
      "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.4215}\n"
     ]
    }
   ],
   "source": [
    "## summarize all 3\n",
    "print(\"String: \" + practice_listing + \" scored as:\\n\" + str(sentiment_example))\n",
    "print(\"String: \" + practice_listing_2 + \" scored as:\\n\" + str(sentiment_example_2))\n",
    "print(\"String: \" + practice_listing_3 + \" scored as:\\n\" + str(sentiment_example_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the dictionary with manually-added words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sent_obj.lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lexicon is a dictionary where the key\n",
    "## is the word\n",
    "## the value is the score (negative = negative)\n",
    "## here, i'm benchmarking the negativity of the\n",
    "## rodents to the negativity of the word aversion\n",
    "sent_obj.lexicon['aversion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After lexicon update: NICE AND COZY LITTLE APT AVAILABLE. HAS RATS THOUGH. scored as:\n",
      "{'neg': 0.228, 'neu': 0.551, 'pos': 0.22, 'compound': -0.0258}\n"
     ]
    }
   ],
   "source": [
    "## create a dictionary with \n",
    "## negative scores for pests\n",
    "pest_words = {\n",
    "    'rat': -1.9,\n",
    "    'rats': -1.9,\n",
    "    'mice': -1.9,\n",
    "    'mouse': -1.9,\n",
    "    'roach': -1.9,\n",
    "    'cockroach': -1.9\n",
    "}\n",
    "\n",
    "## initiate new sentiment object\n",
    "## so that we don't alter old one\n",
    "## use.update to add new words\n",
    "new_si = SentimentIntensityAnalyzer()\n",
    "new_si.lexicon.update(pest_words)\n",
    "\n",
    "## try re-scoring the third example\n",
    "## see negative\n",
    "print(\"After lexicon update: \" + practice_listing_3 + \" scored as:\\n\" + \\\n",
    "      str(new_si.polarity_scores(practice_listing_3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qss20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
